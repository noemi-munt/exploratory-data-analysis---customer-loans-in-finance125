{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Financial Loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to gain insights into the portfolio of a loan company. In this notebook, we will be covering how the data is cleaned and prepared for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and classes\n",
    "from loan_data_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Familiar With the Data\n",
    "The raw data was obtained by connecting to a RDS Database and saving the data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw loan data into a DataFrame\n",
    "loan_data = pd.read_csv('loans_data_raw.csv')\n",
    "\n",
    "# Create a copy of the DataFrame for backup\n",
    "loan_data_copy = loan_data.copy()\n",
    "\n",
    "# Create an instance of DataFrameInfo for data exploration\n",
    "df_info = DataFrameInfo(loan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame:\n",
      "(54231, 43)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(df_info.print_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we are working with a medium sized dataset with over 50,000 records, and 43 variables being tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Data Types\n",
    "First we will check the dataframe and then decide which columns will have to change data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Descriptions:\n",
      "id                               int64\n",
      "member_id                        int64\n",
      "loan_amount                      int64\n",
      "funded_amount                  float64\n",
      "funded_amount_inv              float64\n",
      "term                            object\n",
      "int_rate                       float64\n",
      "instalment                     float64\n",
      "grade                           object\n",
      "sub_grade                       object\n",
      "employment_length               object\n",
      "home_ownership                  object\n",
      "annual_inc                     float64\n",
      "verification_status             object\n",
      "issue_date                      object\n",
      "loan_status                     object\n",
      "payment_plan                    object\n",
      "purpose                         object\n",
      "dti                            float64\n",
      "delinq_2yrs                      int64\n",
      "earliest_credit_line            object\n",
      "inq_last_6mths                   int64\n",
      "mths_since_last_delinq         float64\n",
      "mths_since_last_record         float64\n",
      "open_accounts                    int64\n",
      "total_accounts                   int64\n",
      "out_prncp                      float64\n",
      "out_prncp_inv                  float64\n",
      "total_payment                  float64\n",
      "total_payment_inv              float64\n",
      "total_rec_prncp                float64\n",
      "total_rec_int                  float64\n",
      "total_rec_late_fee             float64\n",
      "recoveries                     float64\n",
      "collection_recovery_fee        float64\n",
      "last_payment_date               object\n",
      "last_payment_amount            float64\n",
      "next_payment_date               object\n",
      "last_credit_pull_date           object\n",
      "collections_12_mths_ex_med     float64\n",
      "mths_since_last_major_derog    float64\n",
      "policy_code                      int64\n",
      "application_type                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Describe the columns to understand their data types\n",
    "print(\"Column Descriptions:\")\n",
    "print(df_info.describe_columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see the following columns are object data type which should be converted to __category__ data type:\n",
    "\n",
    "- term\n",
    "- home_ownership\n",
    "- verification_status\n",
    "\n",
    "and the following will be converted to an __ordered category__ data type.\n",
    "- employment_length\n",
    "- grade\n",
    "- sub_grade\n",
    "\n",
    "The date columns from object to __datetime__ data type, which includes the following:\n",
    "\n",
    "- issue_date\n",
    "- earliest_credit_line\n",
    "- last_payment_date\n",
    "- next_payment_date\n",
    "- last_credit_pull_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m date_column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearliest_credit_line\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_payment_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_payment_date\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_credit_pull_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m df_transform\u001b[38;5;241m.\u001b[39mconvert_to_date(date_column_names)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "df_transform = DataTransform(loan_data)\n",
    "\n",
    "# Convert columns to category type\n",
    "category_column_names = [\"term\",\"grade\",\"sub_grade\",\"employment_length\",\"home_ownership\",\"verification_status\"]\n",
    "df = df_transform.convert_to_category(category_column_names)\n",
    "\n",
    "# Convert columns to ordered category type\n",
    "col_order = {\n",
    "    \"employment_length\": [\"< 1 year\", \"1 year\", \"2 years\", \"3 years\", \"4 years\",\"5 years\", \"6 years\",\"7 years\", \"8 years\",\"9 years\", \"10+ years\"],\n",
    "    \"grade\": [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"],\n",
    "    \"sub_grade\": [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"D1\",\"D2\",\"D3\",\"D4\",\"D5\",\"E1\",\"E2\",\"E3\",\"E4\",\"E5\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\"]\n",
    "}\n",
    "df = df_transform.convert_to_ordered_category(col_order)\n",
    "\n",
    "# Convert columns to datetime type\n",
    "date_column_names = [\"issue_date\",\"earliest_credit_line\",\"last_payment_date\",\"next_payment_date\",\"last_credit_pull_date\"]\n",
    "df = df_transform.convert_to_date(date_column_names)\n",
    "\n",
    "print(df.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distinct values and mode of catagorical data\n",
    "cat_stats = df_info.count_distinct_values_and_mode()\n",
    "\n",
    " # Convert stats dictionary to a DataFrame for better readability\n",
    "cat_stats_df = pd.DataFrame(cat_stats).T  # Transpose the table so columns are rows\n",
    "cat_stats_df.index.name = 'Variable' \n",
    "print(cat_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we know all the loans are personal loans, most are for mortgages and fully paid. The loan terms can either be 60 or 36 months, the most common being 36 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and format statistics for numeric columns\n",
    "stats_dict = df_info.extract_statistics() # This returns a dictionary with Pandas Series\n",
    "formatted_stats_dict = {key: stats_dict[key].apply(lambda x: f\"{x:,.2f}\") for key in stats_dict}\n",
    "\n",
    " # Convert stats dictionary to a DataFrame for better readability\n",
    "num_stats_df = pd.DataFrame(formatted_stats_dict) \n",
    "num_stats_df.index.name = 'Variable' \n",
    "print(num_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Averages\n",
    "- loan_amouunt £13,333.08\n",
    "- total_payment £12,079.21 which is below the loan_amount, suggesting they are working at a loss.\n",
    "- int_rate 13.51% this is high interest rate for a personal loan.\n",
    "\n",
    "Some strange results can be seen by these preliminary calculations, the standard deviation of the loan amount is 8,082.20. With some cleaning of the data,  more nuanced insights that we can have confidence in can be gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values\n",
    "Handling null values is important as they can effect the accuracy of future analysis and cause errors in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null Value Summary\n",
    "\n",
    "First let us calculate the count and percentage of null values in each column to give us an idea of what we are working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count null values\n",
    "null_summary = df_info.count_null_values()\n",
    "print(\"Null Value Summary:\")\n",
    "print(null_summary)\n",
    "print(f\"\\n Number of columns with null values {(null_summary['count'] != 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 variables with null values ranging from 86% to less than %, each one will have to be handled on a case by case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Columns\n",
    "\n",
    "Variables with a null value percentage greater than 50% will be dropped as dropping the missing samples would reduce the dataset too much. This includes:\n",
    "- mths_since_last_delinq\n",
    "- mths_since_last_record  \n",
    "- next_payment_date \n",
    "- mths_since_last_major_derog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of DataFrameTransform using the transformed data\n",
    "df_transform = DataFrameTransform(loan_data)\n",
    "\n",
    "thresh_max = 50  # threshold percentage \n",
    "df_transform.drop_columns(null_summary,thresh_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Rows\n",
    "\n",
    "The following variables have null values percentages of below 1%:\n",
    "- last_payment_date (0.13%)\n",
    "- last_credit_pull_date (0.01%)\n",
    "- collections_12_mths_ex_med (0.09%)\n",
    "\n",
    "Removing the null values of these variables will not reduce the size of the data set too much so it is easier to  drop the rows with null nvalues, additionally the variables with datetime data type (last_payment_date and last_credit_pull_date) would be difficult to impute and not provide much more insight.\n",
    "\n",
    "The employment_length variable has a null percentage of 3.9% which is worth considering imputing. First lets look at the spread of the data by plotting a histogram to check the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'employment_length' column\n",
    "df_sorted = df_transform.df.sort_values(by='employment_length')\n",
    "x_val = df_sorted['employment_length']\n",
    "# Plot the histogram\n",
    "employment_length_inst = Plotter(df_sorted)\n",
    "employment_length_hist = employment_length_inst.histogram(x_val,\"Employment Length\", \"Employment Length Histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that the mode is 10+ years of employment, which makes sense as that captures more years than one year. Since it does not have a normal distribution imputing it with the mode would introduce bias and the employment_length cannot be infered from other data in this dataset. The best option is to drop the rows with null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a threshold of 4% ensures the employment_length, last_payment_date, last_credit_pull_date, collections_12_mths_ex_med are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_min = 4 \n",
    "\n",
    "df_transform.drop_rows(null_summary,thresh_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring missing data\n",
    "\n",
    "Funded Amount\n",
    "\n",
    "The funded_amount is the same as the loan amount so we can imptute the data using the corresponding loan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing funded_amount values with corresponding loan_amount values\n",
    "df_transform.df['funded_amount'] = df_transform.df['funded_amount'].fillna(value=df_transform.df['loan_amount'])\n",
    "null_summary.loc['funded_amount',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term\n",
    "\n",
    "The term can only be a 36 month contract of 60 month contract, the missing term values can be calculated by multiplying the instalment by 36 or 60 and choosing the one closer to the loan amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predited loan amounts\n",
    "loan_36 = df_transform.df['instalment'] * 36\n",
    "loan_60 = df_transform.df['instalment'] * 60\n",
    "# Calculate the difference between the real loan and predicted loan\n",
    "dif_36 =  df_transform.df['loan_amount'] - loan_36\n",
    "dif_60 =  df_transform.df['loan_amount'] - loan_60\n",
    "# Create a DataFrame to display the series and the comparison result\n",
    "df_comparison = pd.DataFrame({\n",
    "    '36': dif_36,\n",
    "    '60': dif_60\n",
    "})\n",
    "\n",
    "# Determine if the term is 36 or 60 months\n",
    "df_comparison['term'] = df_comparison.apply(lambda row: '60' if row['36'] > 0 else '36', axis=1)\n",
    "\n",
    "df_check = pd.DataFrame({\n",
    "    \"calc_term\": df_comparison['term'],\n",
    "    'real_term' : df_transform.df['term'],\n",
    "\n",
    "})\n",
    "print(df_check.head(9))\n",
    "\n",
    "df_transform.df['term'] = df_comparison['term']\n",
    "\n",
    "print('number of term null values',df_transform.df['term'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interest rate can be imputed using the mean/median as the null percentage is less than 10%, as the bias and increase of variance will be negligble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_int_rate = df_transform.df['int_rate'] #x axis values\n",
    "data_vis = Plotter(df_transform.df)\n",
    "int_rate_hist = data_vis.histogram(x_int_rate,\"Interest Rate\", \"Interest Rate Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The median of int_rate is {df_transform.df[\"int_rate\"].median()}')\n",
    "print(f'The mean of int_rate is {df_transform.df[\"int_rate\"].mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interest rate has a right skewed distribution, which typically increases the mean as seen in our calculations. A median imputation is  the better option in this scenario.\n",
    "\n",
    "It could be interesting to see a correlation matrix to determine which variable is mostly strongly correlated and doing a regression based off that and comparing the result to the median regression, testing wheather our assumption that a null percentage value of less than 10% is an acceptable threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute int_rate with median:\n",
    "df_transform.df['int_rate'] = df_transform.impute_median('int_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Change in Null Values\n",
    "\n",
    "Using the missingno package we are able to visualise the null values in the dataset. Every white line is a missing record.\n",
    "\n",
    "Snapshot of dataset before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(loan_data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snapshot of dataset after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_transform.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are no null recrods left in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "df_transform.df.to_csv('loans_data_clean.csv', index=False)\n",
    "print(\"Data saved to loans_data_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Skewewd Data\n",
    "Skewed data can lead to biased models and inaccurate results, so it's important to address this issue before proceeding with any analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Skewed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select numeric columns to be analysed for skewness\n",
    "numeric_cols = ['loan_amount', 'funded_amount', 'funded_amount_inv',\n",
    "       'int_rate', 'instalment', 'annual_inc', 'dti', 'delinq_2yrs', \n",
    "       'inq_last_6mths', 'open_accounts', 'total_accounts', 'out_prncp',\n",
    "       'out_prncp_inv', 'total_payment', 'total_payment_inv',\n",
    "       'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee','last_payment_amount',\n",
    "       'collections_12_mths_ex_med']\n",
    "\n",
    "# Columns not to be analysed for skewness\n",
    "non_skew_features = [col for col in df_transform.df.columns if col not in numeric_cols] \n",
    "\n",
    "#facetgrid plot\n",
    "sns.set_theme(font_scale=0.7)\n",
    "f = pd.melt(df_transform.df, value_vars=numeric_cols)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False)\n",
    "g = g.map(sns.histplot, \"value\", kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate skewness and determine a threshold for the skewness of the data, over which a column will be considered skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transform.df \n",
    "# Calculate skewness\n",
    "skewness = df[numeric_cols].skew()\n",
    "\n",
    "print(f\"Skewness values:\\n{skewness}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables have a positive skew. A normal distribution has a skew of 0, suggesting that our data is not normally distributed. If you're dealing with normally distributed data, a skewness threshold of ±0.5 is often used. For less strict symmetry, a threshold of ±1 is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold\n",
    "threshold = 1  # Adjust as needed\n",
    "\n",
    "# Identify highly skewed columns\n",
    "highly_skewed_cols = skewness[abs(skewness) > threshold]\n",
    "\n",
    "print(\"\\nHighly skewed columns:\\n\", highly_skewed_cols)\n",
    "col_names = highly_skewed_cols.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform transformations \n",
    "\n",
    "on these columns to determine which transformation results in the biggest reduction in skew. \n",
    "Create the the method to transform the columns in your DateFrameTransform class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = df_transform.skew_transform(col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers\n",
    "\n",
    "Outliers are samples which lie outside the range of expected values. They can be spotted by visual inspection and calculated by using methods such as the Interquartile Range Method. However for variables that have many zero values such as the delinq_2years which measures how many delinquencies a customer has had in the past 2 years. Most customers have no delinquencies so calculating outliers for these variables must be adapted to account for the zero values.\n",
    "\n",
    "#### Visualising Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facetgrid plot\n",
    "sns.set_theme(font_scale=0.7)\n",
    "f = pd.melt(df_transform.df, value_vars=numeric_cols)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False)\n",
    "g = g.map(sns.boxplot, \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show the outliers as the circles outside of the whiskers of the boxplot. We can see for variables that have many zero values that there are many outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outliers\n",
    "\n",
    "The variables have been split into two groups. The first group will have outliers removed using the traditional IQR Method where the threshold is +/- 1.5 *IQR. The second group is for variables that have meaningful zero values, and the thresholds are relaxed  to +/- 3 * IQR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort columns into different methods to remove outliers\n",
    "IQR_cols = ['loan_amount', 'funded_amount', 'funded_amount_inv',\n",
    "       'int_rate', 'instalment', 'annual_inc', 'dti', 'open_accounts', 'total_accounts',\n",
    "       'total_payment', 'total_payment_inv',\n",
    "       'total_rec_prncp', 'total_rec_int', \n",
    "       'last_payment_amount',\n",
    "       ]\n",
    "\n",
    "for col in IQR_cols:\n",
    "    df_no_outliers = df_transform.remove_outliers_iqr(col)\n",
    "    \n",
    "#'out_prncp', 'out_prncp_inv' , zero means nothing outstanding\n",
    "\n",
    "IQR_mod = [ 'delinq_2yrs', 'inq_last_6mths',\n",
    "           'out_prncp', 'out_prncp_inv',\n",
    "           'total_rec_late_fee', 'recoveries',\n",
    "           'collection_recovery_fee',\n",
    "           'collections_12_mths_ex_med'\n",
    "           ]\n",
    "\n",
    "for col in IQR_mod:\n",
    "    df_no_outliers = df_transform.remove_outliers_modified_iqr(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_original = pd.read_csv('loans_data_clean.csv') # Copy before removing outliers/skew transformations\n",
    "\n",
    "# Apply outlier removal (Assuming remove_outliers_iqr is implemented in df_transform)\n",
    "\n",
    "col = 'loan_amount'\n",
    "df_no_outliers_copy = df_no_outliers.copy()\n",
    "\n",
    "# Prepare Data for Boxplot (Melt for better visualization)\n",
    "df_original[\"Outlier_Status\"] = \"Before Removal\"\n",
    "df_no_outliers_copy[\"Outlier_Status\"] = \"After Removal\"\n",
    "\n",
    "df_combined = pd.concat([df_original, df_no_outliers_copy])  # Combine both datasets\n",
    "\n",
    "# Plot Boxplots\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Outlier_Status\", y=col, data=df_combined)\n",
    "\n",
    "# Add labels\n",
    "plt.title(f\"Comparison of { col } Before and After Outlier Removal\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Overly Correlated Columns\n",
    "\n",
    "\"Highly correlated columns in a dataset can lead to multicollinearity issues, which can affect the accuracy and interpretability of models built on the data.\"\n",
    "\n",
    "#### Visualise the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for numeric columns only\n",
    "numeric_df = df_no_outliers.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two clusters of very high correlation (>0.9). The first in the top right between loan_amount, funded_amount, funded_amount_inv, and instalment. This makes sense as the latter three are based on the first. The second cluster in the bottom left is between total_payment, total_payment_inv, total_rec_prncp which is expected. Another obvious insight from the heatmap is that out_prncp and put_prncp_inv are perfectly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Highly Correlated Columns\n",
    "\n",
    "Based on the heatmap I will choose a threshold of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a correlation threshold\n",
    "threshold = 0.9\n",
    "\n",
    "# Find pairs of columns with correlation above the threshold\n",
    "high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "\n",
    "# Filter out pairs where the correlation is not with itself\n",
    "high_corr_pairs = [(corr_matrix.columns[x], corr_matrix.columns[y]) \n",
    "                   for x, y in zip(*high_corr_pairs) \n",
    "                   if x != y and x < y]\n",
    "\n",
    "# Display the highly correlated pairs\n",
    "print(\"Highly correlated pairs:\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Decide Which Columns to Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: {'total_rec_prncp', 'out_prncp_inv', 'funded_amount_inv', 'total_payment_inv', 'instalment', 'funded_amount', 'member_id'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of columns to remove\n",
    "columns_to_remove = set()\n",
    "\n",
    "# Iterate through the highly correlated pairs and remove the second columns\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    columns_to_remove.add(col2)\n",
    "\n",
    "# Display the columns to remove\n",
    "print(\"Columns to remove:\", columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threshold of 0.9 removes 7 columns:\n",
    "funded_amount, funded_amount_inv, instalment, total_payment_inv, total_rec_prncp, out_prncp_inv, member_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns from the DataFrame\n",
    "df_reduced = df_no_outliers.drop(columns=columns_to_remove)\n",
    "\n",
    "# Display the reduced DataFra\n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is ready to be used for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
